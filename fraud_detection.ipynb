{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 - Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be looking at credit card information to create an automated fraud detector. This model will immediately identify the transaction as fraudulent or genuine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fraud_train = pd.read_parquet(\n",
    "    \"https://cs307.org/lab/data/fraud-train.parquet\",\n",
    ")\n",
    "fraud_test = pd.read_parquet(\n",
    "    \"https://cs307.org/lab/data/fraud-test.parquet\",\n",
    ")\n",
    "# create X and y for train\n",
    "X_train = fraud_train.drop(\"Fraud\", axis=1)\n",
    "y_train = fraud_train[\"Fraud\"]\n",
    "\n",
    "# create X and y for test\n",
    "X_test = fraud_test.drop(\"Fraud\", axis=1)\n",
    "y_test = fraud_test[\"Fraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137250</th>\n",
       "      <td>-0.514509</td>\n",
       "      <td>0.899378</td>\n",
       "      <td>1.627215</td>\n",
       "      <td>-0.142250</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>-0.235422</td>\n",
       "      <td>0.482540</td>\n",
       "      <td>0.247403</td>\n",
       "      <td>-0.562327</td>\n",
       "      <td>-0.166813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143290</td>\n",
       "      <td>-0.390205</td>\n",
       "      <td>0.030719</td>\n",
       "      <td>0.184779</td>\n",
       "      <td>-0.348711</td>\n",
       "      <td>0.073253</td>\n",
       "      <td>0.273217</td>\n",
       "      <td>0.107938</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132979</th>\n",
       "      <td>-0.813568</td>\n",
       "      <td>-0.373893</td>\n",
       "      <td>1.152977</td>\n",
       "      <td>-0.449774</td>\n",
       "      <td>-3.868866</td>\n",
       "      <td>2.780636</td>\n",
       "      <td>3.654192</td>\n",
       "      <td>-0.672442</td>\n",
       "      <td>0.753230</td>\n",
       "      <td>-0.662803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376783</td>\n",
       "      <td>-0.004239</td>\n",
       "      <td>0.074801</td>\n",
       "      <td>0.124238</td>\n",
       "      <td>-0.448493</td>\n",
       "      <td>0.861423</td>\n",
       "      <td>-0.093639</td>\n",
       "      <td>-0.711632</td>\n",
       "      <td>798.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78191</th>\n",
       "      <td>-2.443142</td>\n",
       "      <td>3.258831</td>\n",
       "      <td>-0.791511</td>\n",
       "      <td>0.223548</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>-1.263044</td>\n",
       "      <td>1.220214</td>\n",
       "      <td>-0.418068</td>\n",
       "      <td>1.860453</td>\n",
       "      <td>4.184883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348587</td>\n",
       "      <td>0.531679</td>\n",
       "      <td>0.058990</td>\n",
       "      <td>0.371638</td>\n",
       "      <td>-0.207398</td>\n",
       "      <td>-0.505837</td>\n",
       "      <td>0.524542</td>\n",
       "      <td>-0.343895</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113508</th>\n",
       "      <td>-0.397300</td>\n",
       "      <td>0.922104</td>\n",
       "      <td>1.224699</td>\n",
       "      <td>-0.334974</td>\n",
       "      <td>0.322603</td>\n",
       "      <td>-0.117372</td>\n",
       "      <td>0.534683</td>\n",
       "      <td>0.175550</td>\n",
       "      <td>-0.486404</td>\n",
       "      <td>-0.120147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239303</td>\n",
       "      <td>-0.695001</td>\n",
       "      <td>-0.128231</td>\n",
       "      <td>-0.536463</td>\n",
       "      <td>-0.138971</td>\n",
       "      <td>0.107526</td>\n",
       "      <td>0.255644</td>\n",
       "      <td>0.100814</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199222</th>\n",
       "      <td>1.994046</td>\n",
       "      <td>-0.367813</td>\n",
       "      <td>-0.462867</td>\n",
       "      <td>0.338661</td>\n",
       "      <td>-0.485326</td>\n",
       "      <td>-0.241576</td>\n",
       "      <td>-0.590987</td>\n",
       "      <td>0.089319</td>\n",
       "      <td>1.413224</td>\n",
       "      <td>-0.149292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196388</td>\n",
       "      <td>-0.484457</td>\n",
       "      <td>0.421867</td>\n",
       "      <td>0.601393</td>\n",
       "      <td>-0.448014</td>\n",
       "      <td>-0.646256</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>-0.027244</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76424</th>\n",
       "      <td>-0.033583</td>\n",
       "      <td>-0.487672</td>\n",
       "      <td>1.435406</td>\n",
       "      <td>-2.759369</td>\n",
       "      <td>-1.785638</td>\n",
       "      <td>0.402364</td>\n",
       "      <td>-2.306038</td>\n",
       "      <td>-2.287179</td>\n",
       "      <td>0.129717</td>\n",
       "      <td>-1.036412</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.286823</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.301618</td>\n",
       "      <td>0.081215</td>\n",
       "      <td>0.163422</td>\n",
       "      <td>0.252456</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264579</th>\n",
       "      <td>-0.731245</td>\n",
       "      <td>1.151677</td>\n",
       "      <td>0.912393</td>\n",
       "      <td>-0.653540</td>\n",
       "      <td>0.700577</td>\n",
       "      <td>-0.461372</td>\n",
       "      <td>1.093273</td>\n",
       "      <td>-0.126254</td>\n",
       "      <td>-0.339171</td>\n",
       "      <td>-0.467356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368185</td>\n",
       "      <td>-0.991854</td>\n",
       "      <td>-0.376461</td>\n",
       "      <td>-0.661608</td>\n",
       "      <td>0.554237</td>\n",
       "      <td>0.450650</td>\n",
       "      <td>0.076395</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204332</th>\n",
       "      <td>2.135923</td>\n",
       "      <td>-0.714182</td>\n",
       "      <td>-1.842502</td>\n",
       "      <td>-0.587267</td>\n",
       "      <td>0.090754</td>\n",
       "      <td>-0.465733</td>\n",
       "      <td>-0.024834</td>\n",
       "      <td>-0.328747</td>\n",
       "      <td>-0.659832</td>\n",
       "      <td>0.853241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537267</td>\n",
       "      <td>-1.081138</td>\n",
       "      <td>0.196589</td>\n",
       "      <td>0.087797</td>\n",
       "      <td>-0.111583</td>\n",
       "      <td>0.479888</td>\n",
       "      <td>-0.082523</td>\n",
       "      <td>-0.053305</td>\n",
       "      <td>64.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167301</th>\n",
       "      <td>-0.335632</td>\n",
       "      <td>0.939736</td>\n",
       "      <td>-2.961515</td>\n",
       "      <td>-1.224739</td>\n",
       "      <td>4.446891</td>\n",
       "      <td>2.352092</td>\n",
       "      <td>0.789725</td>\n",
       "      <td>0.609460</td>\n",
       "      <td>-0.746845</td>\n",
       "      <td>-1.580751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>-0.428325</td>\n",
       "      <td>0.536043</td>\n",
       "      <td>0.278736</td>\n",
       "      <td>0.680976</td>\n",
       "      <td>-0.069295</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192581</th>\n",
       "      <td>-1.444958</td>\n",
       "      <td>1.264710</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>-0.607371</td>\n",
       "      <td>0.155386</td>\n",
       "      <td>0.264429</td>\n",
       "      <td>-0.246528</td>\n",
       "      <td>0.703092</td>\n",
       "      <td>0.374768</td>\n",
       "      <td>-0.493145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271551</td>\n",
       "      <td>1.072854</td>\n",
       "      <td>-0.449656</td>\n",
       "      <td>0.122150</td>\n",
       "      <td>0.209835</td>\n",
       "      <td>-0.081948</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>-0.131398</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54276 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC01      PC02      PC03      PC04      PC05      PC06      PC07  \\\n",
       "137250 -0.514509  0.899378  1.627215 -0.142250  0.005250 -0.235422  0.482540   \n",
       "132979 -0.813568 -0.373893  1.152977 -0.449774 -3.868866  2.780636  3.654192   \n",
       "78191  -2.443142  3.258831 -0.791511  0.223548  0.007932 -1.263044  1.220214   \n",
       "113508 -0.397300  0.922104  1.224699 -0.334974  0.322603 -0.117372  0.534683   \n",
       "199222  1.994046 -0.367813 -0.462867  0.338661 -0.485326 -0.241576 -0.590987   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "76424  -0.033583 -0.487672  1.435406 -2.759369 -1.785638  0.402364 -2.306038   \n",
       "264579 -0.731245  1.151677  0.912393 -0.653540  0.700577 -0.461372  1.093273   \n",
       "204332  2.135923 -0.714182 -1.842502 -0.587267  0.090754 -0.465733 -0.024834   \n",
       "167301 -0.335632  0.939736 -2.961515 -1.224739  4.446891  2.352092  0.789725   \n",
       "192581 -1.444958  1.264710  0.370500 -0.607371  0.155386  0.264429 -0.246528   \n",
       "\n",
       "            PC08      PC09      PC10  ...      PC21      PC22      PC23  \\\n",
       "137250  0.247403 -0.562327 -0.166813  ... -0.143290 -0.390205  0.030719   \n",
       "132979 -0.672442  0.753230 -0.662803  ... -0.376783 -0.004239  0.074801   \n",
       "78191  -0.418068  1.860453  4.184883  ... -0.348587  0.531679  0.058990   \n",
       "113508  0.175550 -0.486404 -0.120147  ... -0.239303 -0.695001 -0.128231   \n",
       "199222  0.089319  1.413224 -0.149292  ... -0.196388 -0.484457  0.421867   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "76424  -2.287179  0.129717 -1.036412  ... -1.286823  0.824100  0.018890   \n",
       "264579 -0.126254 -0.339171 -0.467356  ... -0.368185 -0.991854 -0.376461   \n",
       "204332 -0.328747 -0.659832  0.853241  ... -0.537267 -1.081138  0.196589   \n",
       "167301  0.609460 -0.746845 -1.580751  ... -0.002116  0.009686 -0.428325   \n",
       "192581  0.703092  0.374768 -0.493145  ...  0.271551  1.072854 -0.449656   \n",
       "\n",
       "            PC24      PC25      PC26      PC27      PC28  Amount  Fraud  \n",
       "137250  0.184779 -0.348711  0.073253  0.273217  0.107938    3.59      0  \n",
       "132979  0.124238 -0.448493  0.861423 -0.093639 -0.711632  798.01      0  \n",
       "78191   0.371638 -0.207398 -0.505837  0.524542 -0.343895    1.79      0  \n",
       "113508 -0.536463 -0.138971  0.107526  0.255644  0.100814    2.69      0  \n",
       "199222  0.601393 -0.448014 -0.646256  0.027632 -0.027244    4.49      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "76424   0.000018  0.301618  0.081215  0.163422  0.252456   10.00      0  \n",
       "264579 -0.661608  0.554237  0.450650  0.076395  0.096252    6.99      0  \n",
       "204332  0.087797 -0.111583  0.479888 -0.082523 -0.053305   64.90      0  \n",
       "167301  0.536043  0.278736  0.680976 -0.069295  0.051891    0.76      0  \n",
       "192581  0.122150  0.209835 -0.081948  0.034733 -0.131398    7.49      0  \n",
       "\n",
       "[54276 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics\n",
    "fraud_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training data has 54,276 samples and 2 features.\n",
    "#### Response\n",
    "\n",
    "**`Fraud`**\n",
    "\n",
    "- `[int64]` status of the transaction. 1 indicates a fraudulent transaction and 0 indicates not fraud, a genuine transaction.\n",
    "\n",
    "#### Features\n",
    "\n",
    "`Amount`\n",
    "\n",
    "- `[float64]` amount (in dollars) of the transaction.\n",
    "\n",
    "\n",
    "`PC01 - PC28`\n",
    "\n",
    "- `[float64]` the 28 principal components that encode information such as location and type of purchase while preserving customer privacy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005803670130444395"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fraud (target) balance\n",
    "fraud_train.groupby('Fraud').agg('count')\n",
    "proportion_genuine = 53961/54276\n",
    "proportion_fraud = 315/54276\n",
    "proportion_fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of genuine transactions are around 99.5% of the observations. The proportion of fraudulent transactions are around 0.006% of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fraud</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53961.0</td>\n",
       "      <td>88.065104</td>\n",
       "      <td>241.451144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>21.80</td>\n",
       "      <td>75.97</td>\n",
       "      <td>10199.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315.0</td>\n",
       "      <td>110.947016</td>\n",
       "      <td>254.978960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.99</td>\n",
       "      <td>99.99</td>\n",
       "      <td>2125.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count        mean         std  min  25%    50%    75%       max\n",
       "Fraud                                                                   \n",
       "0      53961.0   88.065104  241.451144  0.0  5.5  21.80  75.97  10199.44\n",
       "1        315.0  110.947016  254.978960  0.0  1.0   6.99  99.99   2125.87"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_summary = fraud_train.groupby('Fraud')['Amount'].describe()\n",
    "fraud_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exploratory visualization\n",
    "sns.barplot(x='Fraud', y = 'Amount', data=fraud_train)\n",
    "\n",
    "plt.title('Fraud Distribution (0 vs 1)')\n",
    "plt.xlabel('Fraud (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the relationship between fraudulent transactions and the amount for these transactions, we created a barplot to show the distribution of fraud vs amount. It appears that fraudulent transactions (1=fraudulent) have a higher maximum amount value compared to non-fraudulent transactions. Due to the difference in distributions between fraudulent and non-fraudulent transactions, we see that there is evidence suggesting that there fraudulent transactions typically have higher amounts for their transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for ML\n",
    "# helper function to print cross-validation results\n",
    "def print_metric_scores(grid, metric):\n",
    "    cv_results = grid.cv_results_\n",
    "    best_index = grid.best_index_\n",
    "    mean_score = cv_results[f\"mean_test_{metric}\"][best_index]\n",
    "    std_score = cv_results[f\"std_test_{metric}\"][best_index]\n",
    "    print(f\"CV {metric} (mean ± std): {mean_score:.3f} ± {std_score:.3f}\")\n",
    "\n",
    "numeric_features = ['Amount']\n",
    "target = 'Fraud'\n",
    "#preprocessing for numerical features\n",
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    #StandardScaler(),\n",
    ")\n",
    "\n",
    "#preprocessing for categorical features\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"infrequent_if_exist\"),\n",
    ")\n",
    "\n",
    "# create general preprocessor\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "mod = DecisionTreeClassifier(random_state=42)\n",
    "# weights for helping with imbalance\n",
    "weights_list = [\n",
    "    {0: 1, 1: 1},\n",
    "    {0: 1, 1: 2},\n",
    "    \"balanced\",\n",
    "]\n",
    "\n",
    "# define scoring metrics\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "    \"f1\": make_scorer(fbeta_score, beta=1),\n",
    "}\n",
    "\n",
    "# define parameter grid\n",
    "mod_param_grid = {\n",
    "    \"max_depth\": [2,5,10],\n",
    "    \"class_weight\": weights_list,\n",
    "}\n",
    "\n",
    "# setup grid search\n",
    "mod_grid = GridSearchCV(\n",
    "    mod,\n",
    "    mod_param_grid,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    refit=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found with cross-validation:\n",
      "{'class_weight': {0: 1, 1: 2}, 'max_depth': 2}\n",
      "\n",
      "CV accuracy (mean ± std): 0.998 ± 0.000\n",
      "CV precision (mean ± std): 0.864 ± 0.054\n",
      "CV recall (mean ± std): 0.819 ± 0.050\n",
      "CV f1 (mean ± std): 0.839 ± 0.034\n",
      "\n",
      "Test Accuracy: 0.9985997494288451\n",
      "Test Precision: 0.9285714285714286\n",
      "Test Recall: 0.8227848101265823\n",
      "Test F1 Score: 0.87248322147651\n"
     ]
    }
   ],
   "source": [
    "# report model metrics\n",
    "# fit model specified by grid search\n",
    "mod_grid.fit(X_train, y_train)\n",
    "\n",
    "# print the best parameters and cross-validation metrics\n",
    "print(\"\")\n",
    "print(f\"Best parameters found with cross-validation:\")\n",
    "print(mod_grid.best_params_)\n",
    "print(\"\")\n",
    "print_metric_scores(mod_grid, \"accuracy\")\n",
    "print_metric_scores(mod_grid, \"precision\")\n",
    "print_metric_scores(mod_grid, \"recall\")\n",
    "print_metric_scores(mod_grid, \"f1\")\n",
    "\n",
    "# make predictions on the test set using the best model\n",
    "y_pred = mod_grid.predict(X_test)\n",
    "\n",
    "# calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = fbeta_score(y_test, y_pred, beta=1)\n",
    "\n",
    "# print test metrics\n",
    "print(\"\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After providing multiple weights and different max depth values for our Decision Tree model, we determined that the best model for this dataset with scores for a balance between recall, accuracy, precision, and f1beta to be a decision tree with max depth of 2 and using twice the weights for fraudulent transactions.\n",
    "\n",
    "Our resulting test metrics are:\n",
    "\n",
    "Test Accuracy: 0.9985997494288451\n",
    "\n",
    "Test Precision: 0.9285714285714286\n",
    "\n",
    "Test Recall: 0.8227848101265823\n",
    "\n",
    "Test F1 Score: 0.87248322147651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Metric  Count\n",
      "0   True Positive (TP)     65\n",
      "1   True Negative (TN)  13485\n",
      "2  False Positive (FP)      5\n",
      "3  False Negative (FN)     14\n"
     ]
    }
   ],
   "source": [
    "# summary figure\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "confusion_df = pd.DataFrame({\n",
    "    'Metric': ['True Positive (TP)', 'True Negative (TN)', 'False Positive (FP)', 'False Negative (FN)'],\n",
    "    'Count': [TP, TN, FP, FN]\n",
    "})\n",
    "\n",
    "print(confusion_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize our results, a confusion matrix was created to show how well our model can predict future fraudulent transactions. From the table output that is shown, we can see the true positive, true negative, false positive, and false negative counts of our predicted and actual fraudulent transactions. \n",
    "\n",
    "We would like to put more emphasis on creating a bigger coverage for fraudulent transactions since losing money and being notified of it is much more valuable than being notified of a false fraudulent report. It is better to be cautious and aware than unware of lost funds.\n",
    "\n",
    "In regards to that concern, we can see that we have 14 false negatives out of 13,569 test observations. This means that we fail to detect 0.1% of transactions that are fraudulent out of all transactions. This is concerning because we should develop a model that covers more fraudulent transactions. Although the percentage is small, credit card transactions on the daily are numerous and will still have great gravity despite the small number of failed fraud detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fraud.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize model\n",
    "from joblib import dump\n",
    "dump(mod_grid, \"fraud.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite fitting various weights and max depth values for our Decision Tree model, it is clear from our summary figure and analysis that we should not use this model. Credit card fraud is a very serious problem that can cause irreversible damage to the client if gone undetected. It is at the upmost importance that the bank can trace the scammer as soon as possible to recover funds. Based on our model's confusion matrix, we can see that we fail to detect 0.1% of fraudulent transactions out of all transactions. Like previously stated, although this percentage is small, the number of real life credit card transactions are many. The number of undetected fraudulent transactions, while using this model, will be much higher than expected.\n",
    "\n",
    "In order to remedy this, we should increase the number of fraudulent transactions in our dataset (postives) so that the number of false negatives (undetected fraudulent transactions) decrease. It is not that concerning for a client to be notified incorrectly of a fraud than someone to not be notified at all and there is truly a fraudulent transaction occurring. The person in the situation prior would lose no money (just some sanity) while the other could be losing thousands if the bank fails to retrieve or identify the fraud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs307-content",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
